# Implement simple gradient descent for linear regression.
### You will have a simple dataset containing 20 (x, y) points, you should implement a simple gradient decent in Python to calculate best fit for the hypothesis function. These examples are taken from Week 1 in Machine Learning course by Andrew Ng.

**Your [dataset](dataset.csv) is in this folder, named dataset.csv**

|Name| Function|
|:------:| :--------------:|
|**The Hypothesis function** | ![h(x)=theta0 + theta1 * x](equations/hypothesis.png) |
|**The cost function** | ![J(theta0, theta1) = (1/2*m)*sum "from 1 to m" of ((h(x subscript i) - y)^2)](equations/cost_function.png)|
|**Gradient Descent** |![no alt text](equations/gradient_descent.png) |

* You don't need to train a model or anything just pure math.
* Your output should be theta0 and theta1, and a plot of the model.
* You should only use python standard library except matplotlib.
* If you find anything hard, you can revise the video associated with it in the course, then in Discord.
